#!/usr/bin/env bash
set -o nounset
set -o errexit

savedir=${WWWSAVEDIR:-"$HOME/www"}
homepage=${WWWHOMEPAGE:-"$savedir/homepage.html"}
useragent=${USER_AGENT:-"Mozilla/5.0 (compatible; Konqueror/3.2; Linux)"}

use_js=true
use_docs=true
use_fonts=true
use_images=true

# wget options
options+=(--directory-prefix="$savedir")     # directory to store mirroring
options+=(--quiet --show-progress)      # less spam
options+=(--progress=bar:noscroll)      # try to show full path
options+=(--force-directories)          # always use directories
options+=(--execute=robots=off)         # ignore robots.txt
options+=(--user-agent="$useragent")
options+=(--no-check-certificate)       # ignore certificates
# Websites with pages like `example.com/foo` and `example.com/foo/bar.html`
# are not handled well by Wget. For now the current workaround is to use
# --adjust-extension, which changes downloads foo as foo.html, but that is
# fragile. Preferably, it should be saved as foo/index.html instead.
# maybe add an adjust-extensionless which does that.
options+=(--adjust-extension)           # fix extensions
options+=(--wait=0.5 --random-wait)     # wait a little
options+=(--tries=100 --timeout=60)     # tries a lot and timeout after a minute
options+=(--timestamping)               # dont redownload files that havnt changed
options+=(--convert-links)              # convert links for local use
options+=(--page-requisites)            # download all prerequisites for local use
options+=(--span-hosts)                 # get prerequisites from other sites
options+=(--rejected-log=$savedir/log)  # log rejections
options+=(--ignore-case)                # ignore case for resource extensions
if [ "$(basename "$0")" = wwwmirror ]   # use mirror if this script is called with that name
then options+=(--mirror)
fi

# handle, arguments, and seperate out urls and wget options
for arg
do
	if [[ "$arg" = "--no-js" ]];     then use_js=false
	elif [[ "$arg" = "--no-fonts" ]]; then use_fonts=false
	elif [[ "$arg" = "--no-img" ]];  then use_images=false
	elif [[ "$arg" = "--no-doc" ]];  then use_docs=false
	elif [[ "$arg" = -* ]]
	then options+=("$arg")
	else urls+=("$arg")
	fi
done

# foreign resources to save
add_to_arr() {
	local c="$1"; shift
	if "$c"; then accept+=("$@"); else reject+=("$@"); fi
}
accept+=(css) # always get css
add_to_arr "$use_js"     js
add_to_arr "$use_fonts"  woff ttf  eot
add_to_arr "$use_images" png  jpeg jpg  svg gif bmp
add_to_arr "$use_docs"   pdf  epub djvu txt
accept_regex="$(IFS='|'; printf '%s' "${accept[*]}")"
reject_list="$(IFS=','; printf '%s' "${reject[*]}")"

# remove protocol, directories, subdomain and store domains in urls
stripped_urls=($(
	for url in "${urls[@]}"
	do sed -r -e 's_.*://__' -e 's_/.*__' -e 's_^.*\.([^.]*\.[^.]*)$_\1_' <<< "$url"
	done | sort -u
))
#
urls_regex="$(IFS='|'; printf "%s" "${stripped_urls[*]}")"
options+=(--accept-regex="$urls_regex|\.($accept_regex)")
if [ "$reject_list" ]
then options+=(--reject="$reject_list")
fi
# List of always blocked websites.
banned_domains="google.com|wikipedia.org|wikimedia.org"
options+=(--reject-regex="$banned_domains")

# Download all urls
echo wget "${options[@]}" "${urls[@]}"
wget "${options[@]}" "${urls[@]}" || true

# store urls into homepage
for url in "${urls[@]}"
do
	echo "<a href=\"$url\">" >> $homepage
	echo "$url"              >> $homepage
	echo "</a><br>"          >> $homepage
done

# then edit them
: ${EDITOR:=vi}
if [ "$EDITOR" = vim ]
then vim + $homepage # opens homepage at bottom
else "$EDITOR" $homepage
fi
