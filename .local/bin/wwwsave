#!/usr/bin/env bash
set -o nounset
set -o errexit

savedir=${WWWSAVEDIR:-"$HOME/www"}
homepage=${WWWHOMEPAGE:-"$savedir/homepage.html"}
useragent=${USER_AGENT:-"Mozilla/5.0 (compatible; Konqueror/3.2; Linux)"}
# wget options
options+=(--directory-prefix="$savedir")     # directory to store mirroring
options+=(--no-verbose --show-progress) # less spam
options+=(--force-directories)          # always use directories
options+=(--execute=robots=off)         # ignore robots.txt
options+=(--user-agent="$useragent")
options+=(--no-check-certificate)       # ignore certificates
# Websites with pages like `example.com/foo` and `example.com/foo/bar.html`
# are not handled well by Wget. For now the current workaround is to use
# --adjust-extension, which changes downloads foo as foo.html, but that is
# fragile. Preferably, it should be saved as foo/index.html instead.
# maybe add an adjust-extensionless which does that.
options+=(--adjust-extension)           # fix extensions
options+=(--wait=0.5 --random-wait)     # wait a little
options+=(--tries=100 --timeout=60)     # tries a lot and timeout after a minute
options+=(--timestamping)               # dont redownload files that havnt changed
options+=(--convert-links)              # convert links for local use
options+=(--page-requisites)            # download all prerequisites for local use
options+=(--span-hosts)                 # get prerequisites from other sites
options+=(--rejected-log=$savedir/log)  # log rejections
options+=(--ignore-case)                # ignore case for resource extensions
if [ "$(basename "$0")" = wwwmirror ]   # use mirror if this script is called with that name
then options+=(--mirror)
fi

# Wget does not handle page-requisites+mirroring very well.
# it considers pages linked via <a> to be downloaded as page-requisites, and 
# proceeds to download the entire internet. the workaround for this is to use an 
# accept-regex that downloads anything with a resources extension, or from the 
# same domain avoiding the "download the internet" problem while still 
# downloading resources from foriegn domains.

# foreign resources to save
resources+=(js css)                   # common prerequisites
resources+=(woff ttf eot)             # fonts
resources+=(png jpeg jpg svg gif bmp) # graphics
resources+=(pdf epub djvu txt)        # documents
resources_regex=$(IFS='|'; printf "${resources[*]}")

# seperate out urls and wget options
for arg in "$@"
do
	if [[ "$arg" = -* ]]
	then options+=("$arg")
	else urls+=("$arg")
	fi
done

# remove protocal, directories, subdomain and store domains in urls
stripped_urls=($(
	for url in "${urls[@]}"
	do sed -r -e 's_.*://__' -e 's_/.*__' -e 's_^.*\.([^.]*\.[^.]*)$_\1_' <<< "$url"
	done | sort -u
))
urls_regex=$(IFS='|'; printf "${stripped_urls[*]}")
options+=(--accept-regex "$urls_regex|\.($resources_regex)")

# Download all urls
wget "${options[@]}" "${urls[@]}" || true

# store urls into homepage
for url in "${urls[@]}"
do
	echo "<a href=\"$url\">" >> $homepage
	echo "$url"              >> $homepage
	echo "</a><br>"          >> $homepage
done

# then edit them
: ${EDITOR:=vi}
if [ "$EDITOR" = vim ]
then vim + $homepage # opens homepage at bottom
else "$EDITOR" $homepage
fi
