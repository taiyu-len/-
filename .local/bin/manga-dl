#!/usr/bin/env bash
# TODO Rewrite to be more modular
# TODO Save all metadata associated with it
# TODO Better Options
set -o errexit
set -o nounset

# Logging
# shellcheck disable=SC2059
log(){ $quiet || printf "$@" ;}
# shellcheck disable=SC2059
err(){
	{
		printf "$@"
		printf '== %s\n' ${link+link="$link"} ${title+title="$title"} ${site+site="$site"}
	} >&2
}
show_help() {
	log '%s\n' "usage: $(basename "$0") -h[elp] -q[uiet] -d[ebug] <urls>..."
}

# Parse options
debug=false
quiet=false
opts="dqh"

#shellcheck disable=SC2046
set -- $(getopt "$opts" "$@")
while getopts "$opts" opt
do case "$opt" in
	q)  quiet=;; +q) quiet=false;;
	d)  debug=;; +d) debug=false;;
	h) show_help; exit;;
	*) echo "Discarding unknown option: $opt";;
esac done
shift $((OPTIND - 1))

if [ "$#" -eq 0 ]
then show_help; exit
fi

$debug && set -x

# Functions
process_url() {
	local link="$1" # Url we are downloading from
	local site      # Site ID we are getting content from
	local title     # Name of content being downloaded
	local pages     # Number of urls we are downloading
	local savedir   # Directory urls are saved in
	if   prepare_urls
	then download_urls
	fi
}

# Prepare urls
prepare_urls() {
	set_site && generate_urls
}
set_site() {
	case "$link" in
	( *gameofscanlation.moe/*  ) site=gos_moe     ;;
	( *readms.net/*            ) site=mangastream ;;
	( *mngcow.co/*             ) site=mangacow    ;;
	( *webtoons.com/*          ) site=webtoons    ;;
	( *otscans.com/foolslide/* ) site=foolslide   ;;
	( *reader.deathtollscans.* ) site=foolslide   ;;
	( *jaiminisbox.com/reader* ) site=foolslide   ;;
	( *imgur.com/a/*           ) site=imgur       ;;
	( *imgur.com/*             ) site=imgursolo   ;;
	( *mangadex.org/*          ) site=mangadex    ;;
	( * )
		err "Unknown link: '$link'\\n"
		return 1
		;;
	esac
}
generate_urls() {
	# Download initial page to fd 10
	exec 10< <(curl -s "$link" || true)
	# Process site to generate urls to fd11
	"process_$site"
}

# Download Urls
download_urls() {
	log "\\nDownloading $title from $site now. ${pages:-unknown} pages\\n"
	savedir="${SAVEDIR-${SAVEDIR_PREFIX-/tmp}/$site/$title}"
	mkdir -p "$savedir"
	cd       "$savedir"
	case "${rename_images[$site]}" in
	0) download_original ;;
	1) download_numbered ;;
	2) download_prefixed ;;
	esac
	NQDIR=/tmp/manga-nq nq sxiv -r "$savedir" </dev/null >/dev/null 2>&1
	exec 11<&-
}

download_original() {
	local url
	while read_url
	do
		local urls=("$url")
		while read_url -t .5
		do urls+=("$url")
		done
		wget "${urls[@]}" || true
	done
}
download_numbered() {
	local url i=1
	while read_url
	do wget "$url" "-O$(printf "%03d" $i).jpg" && ((++i))
	done
}
download_prefixed() {
	local url i=1
	while read_url
	do wget "$url" "-O$(printf "%03d" $i)_$(basename "$url")" && ((++i))
	done
}

read_url() { IFS= read -r -u11 "$@" url;}
wget() { command wget "${wget_opts[@]}" "--referer=$link" "$@" ;}

# wget opts
cookie_file=$(mktemp)
trap "{ rm '$cookie_file'; }" EXIT
wget_opts=(--tries 0)
wget_opts+=(--timeout 60)
wget_opts+=(--continue)
wget_opts+=(--quiet)
wget_opts+=(--save-cookies "$cookie_file")
wget_opts+=(--load-cookies "$cookie_file")
wget_opts+=(--keep-session-cookies)
if ! $quiet
then
	wget_opts+=(--show-progress)
	if tty > /dev/null
	then wget_opts+=('--progress=bar')
	else wget_opts+=('--progress=dot:mega')
	fi
fi

# Site data
# Regex to extract name from between <title>
declare -A name_regex
name_regex[gos_moe]='(.*)\ \|'
name_regex[mangacow]='(.* - Chapter [^-]*) -'
name_regex[webtoons]='(.*)<\/title>'
name_regex[mangastream]='(.*)\ -'
name_regex[foolslide]='(.*::.*) :: '
name_regex[mangadex]='(.*\(.*\)) - MangaDex'

# if 0, do not rename
# if 1, renumber completely
# if 2, prefix name with number
declare -A rename_images
rename_images[gos_moe]=1
rename_images[mangacow]=0
rename_images[webtoons]=1
rename_images[mangastream]=0
rename_images[foolslide]=0
rename_images[mangadex]=0
# TODO save related metadata.
rename_images[imgur]=2
rename_images[imgursolo]=0

# Site processing functions
# Read in initial webpage, and write out urls
extract_title() {
	title=$(sed -nr -e "/<title>/{s/.*<title>${name_regex[$site]}.*/\\1/;T;p;q}" <&10)
	[ "$title" ] || { err "unable to get name from url\\n"; return 1; }
}
extract_page_count() {
	pages=$(sed -nr "$1" <&10)
	[ "$pages" ] || { err "unable to get page count from url\\n"; return 1; }
}
output_urls_from_args() {
	local urls=("$@")
	pages=${#urls[@]}
	if [ "$pages" -eq 0 ]
	then err "Unable to parse links from $site\\n"; return 1
	else exec 11< <(printf '%s\n' "${urls[@]}")
	fi
}
output_urls_from_pages() {
	exec 11< <(
		for (( i=1; i <= "$pages"; ++i ))
		do curl -s "$1/$i" | sed -nr "$2"
		done
	)
}
output_urls_from_page() {
	local args
	mapfile -t args < <(sed -nr "$1" <&10)
	output_urls_from_args "${args[@]}"
}

# Site Processing function
process_gos_moe() {
	extract_title &&
	output_urls_from_page "/alt=\"$title/{s/.*src=\"([^\"]*)\".*/\\1/;p}"
}
process_webtoons() {
	extract_title &&
	output_urls_from_page '/class="_images"/{s/.*data-url="([^"]*)".*/\1/;p}'
}
process_mangacow() {
	extract_title &&
	exec 10< <(tr ';' '\n' <&10) &&
	output_urls_from_page '/arr_img.push/{s/.*"([^"]*)".*/\1/;p}'
}
process_mangastream() {
	extract_title &&
	extract_page_count '/a href.*Last Page/{s|.*/([0-9]*)">Last Page.*|\1|;p;q}' &&
	output_urls_from_pages "$(dirname "$link")" '/id="manga-page"/{s/.*src="([^"]*)".*/http:\1/;p}'
}
process_foolslide() {
	extract_title &&
	extract_page_count '/tbtitle dropdown_parent dropdown_right/{s/.*>([0-9]*) .*/\1/;p;q}' &&
	output_urls_from_pages "$(dirname "$link")" '\,content/comics/,{s/.*src="([^"]*)".*/\1/;p}'
}
process_imgur() {
	title=$(basename "$link")
	local args
	mapfile -t args < <(perl -ne 'if (/item:/) { s/^.*"images"://; print "https://i.imgur.com/$1$2\n" while s|"hash":"([^"]*)".*?"ext":"([^"]*)"||; last }' <&10)
	output_urls_from_args "${args[@]}"
}
process_imgursolo() {
	title=$(basename "$link")
	local args
	mapfile -t args < <(perl -ne 'if (/item:/) { print "https://i.imgur.com/$1$2\n" if s|"hash":"([^"]*)".*?"ext":"([^"]*)"||; last }' <&10)
	output_urls_from_args "${args[@]}"
}
process_mangadex() {
	extract_title || return 1
	local page webtoon
	# Supress showing page contents in debug
	$debug && set +x
	page="$(cat <&10)"
	$debug && set -x
	# some pages have the mangadex part, others use relative url
	local url_regex='s_.*img.*src="((.*)mangadex.org|)([^"]*)".*_\2mangadex.org/\3_'
	mapfile -t webtoon < <(sed -nr '/class="webtoon/{'"$url_regex"';p}' <<< "$page")
	if [ "${#webtoon[@]}" -gt 0 ]
	then output_urls_from_args "${webtoon[@]}"
	else
		pages=$(sed -nr '/Page 1/{s/.*Page ([0-9]*).*/\1/;p;q}' <<< "$page")
		output_urls_from_pages "$link" '/id="current_page"/{'"$url_regex"';p}'
	fi
}

for url
do process_url "$url"
done
