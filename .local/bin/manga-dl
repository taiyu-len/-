#!/usr/bin/env bash
# TODO Rewrite to be more modular
# TODO Save all metadata associated with it
# TODO Better Options
set -o errexit
set -o nounset

# Logging
# args:   $1= printf format, rest print arguments
# shellcheck disable=SC2059
log() { $quiet || printf "$@" ;}
res=0
# shellcheck disable=SC2059
err() {
	res=1
	{
		printf "$@"
		printf '== %s\n' ${link+"link = $link"} ${title+"title = $title"} ${site+"site = $site"}
	} >&2
}
show_help() {
	log '%s\n' "usage: $(basename "$0") -h[elp] -q[uiet] -d[ebug] <urls>..."
}

# Parse options
debug=false
quiet=false
opts="dqh"

#shellcheck disable=SC2046
set -- $(getopt "$opts" "$@")
while getopts "$opts" opt
do case "$opt" in
	q)  quiet=;; +q) quiet=false;;
	d)  debug=;; +d) debug=false;;
	h) show_help; exit;;
	*) echo "Discarding unknown option: $opt";;
esac done
shift $((OPTIND - 1))

$debug && set -x

# Functions
# args: $1 -> link to download from
process_url() {
	local link="$1" # Url we are downloading from
	local site      # Site ID we are getting content from
	local title     # Name of content being downloaded
	local pages     # Number of urls we are downloading
	local savedir   # Directory urls are saved in
	if   prepare_urls
	then download_urls
	fi
}

# Prepare urls
# exit: 1 if unable to set site, or generate urls; else 0
prepare_urls() {
	set_site && generate_urls
}
# vars: $link is used to determine what site is used
# vars: $site = id of website link
set_site() {
	case "$link" in
	( *gameofscanlation.moe/*    ) site=gos_moe     ;;
	( *readms.net/*              ) site=mangastream ;;
	( *mngcow.co/*               ) site=mangacow    ;;
	( *webtoons.com/*            ) site=webtoons    ;;
	( *otscans.com/foolslide/*   ) site=foolslide   ;;
	( *reader.deathtollscans.*   ) site=foolslide   ;;
	( *jaiminisbox.com/reader*   ) site=foolslide   ;;
	( *reader.championscans.com/*) site=foolslide   ;;
	( *reader.kireicake.com/*    ) site=foolslide   ;;
	( *sensescans.com/*          ) site=foolslide   ;;
	( *imgur.com/a/*             ) site=imgur       ;;
	( *imgur.com/*               ) site=imgursolo   ;;
	( *mangadex.org/*            ) site=mangadex    ;;
	( *read.egscans.com/*        ) site=egscans     ;;
	( *manga.fascans.com/*       ) site=fascans     ;;
	( *merakiscans.com/*         ) site=meraki      ;;
	( *toonily.com/*             ) site=toonily     ;;
	( *leviatanscans.com/*       ) site=toonily     ;;
	( * )
		err "Unknown link: '$link'\\n"
		return 1
		;;
	esac
}
generate_urls() {
	# Download initial page to fd 10
	exec 10< <(curl -s "$link" || true)
	# Process site to generate urls to fd11
	"process_$site"
}

# wget opts
: ${cookie_file:=$(mktemp)}
#shellcheck disable=SC2064
trap "{ rm '$cookie_file'; }" EXIT
wget=(wget)
wget+=(--tries inf)
wget+=(--timeout 60)
wget+=(--no-check-certificate)
wget+=(--continue)
wget+=(--show-progress)
wget+=(--progress=bar:noscroll)
wget+=(--save-cookies "$cookie_file")
wget+=(--load-cookies "$cookie_file")
wget+=(--keep-session-cookies)
wget+=(--quiet)
export NQDIR=/tmp/manga-nq
# Download Urls
download_urls() {
	log "$(tput smso)Downloading $title from $site now. ${pages:-unknown} pages$(tput rmso)\\n"
	savedir="${SAVEDIR-${SAVEDIR_PREFIX-/tmp}/$site/$title}"
	mkdir -p "$savedir"
	cd       "$savedir"
# shellcheck disable=SC2191
	local wget=("${wget[@]}" --referer="$link")
	case "${rename_images[$site]}" in
	0) download_original ;;
	1) download_numbered ;;
	2) download_prefixed ;;
	3) download_mangadex ;;
	esac
	if [ $site = "mangadex" ]
	then nq -c sxiv -V -r "$savedir" # version sort patch
	else nq -c sxiv -r "$savedir"
	fi </dev/null >/dev/null 2>&1
	exec 11<&-
}

download_original() {
	local url
	# shellcheck disable=SC2119
	while read_url
	do
		local urls=("$url")
		while read_url -t 1
		do urls+=("$url"); echo "$url" >> urls
		done
		"${wget[@]}" "${urls[@]}" || true
	done
}
download_numbered() {
	#shellcheck disable=SC2016
	local parallel=(parallel -P 2 --quote --rpl '{#} $_=sprintf("%03d",$job->seq())')
	local url
	local wget=("${wget[@]}" --progress=dot:mega)
	# shellcheck disable=SC2119
	while read_url
	do echo "$url"
	done | tee urls | until "${parallel[@]}" "${wget[@]}" '-O{#}.jpg' '{}'
	do true
	done
}
download_prefixed() {
	local url i=1
	# shellcheck disable=SC2119
	while read_url
	do "${wget[@]}" "$url" "-O$(printf "%03d" $i)_$(basename "$url")" && ((++i)); echo "$url" >> urls
	done
}
download_mangadex() {
	download_original
	# sedname 's/\(^\|[^0-9]\)\([0-9]\.\)/\10\2/' ./* 2>&1 | grep -v change
}

#shellcheck disable=SC2120
read_url() { IFS= read -r -u11 "$@" url;}

# Site data
# Regex to extract name from between <title>
declare -A name_regex
name_regex[gos_moe]='(.*)\ \|'
name_regex[mangacow]='(.* - Chapter [^-]*) -'
name_regex[webtoons]='(.*)<\/title>'
name_regex[mangastream]='(.*)\ -'
name_regex[foolslide]='(.*::.*) :: '
name_regex[mangadex]='(.*) - MangaDex'
name_regex[egscans]='EGScans - Online Manga Viewer - (.* - .*) - Page'
# default title extracter doesnt work for fascans
name_regex[fascans]='/<title>/{n;s/ *(.*) - Page.*/\1/;T;p;q}'
name_regex[meraki]='(.* - .*) - Meraki Scans'
name_regex[toonily]='(.*) - .*'

# if 0, do not rename
# if 1, renumber completely
# if 2, prefix name with number
# if 3, rename mangadex names
declare -A rename_images
rename_images[gos_moe]=1
rename_images[mangacow]=0
rename_images[webtoons]=1
rename_images[mangastream]=0
rename_images[foolslide]=0
rename_images[mangadex]=3
rename_images[egscans]=0
rename_images[fascans]=0
rename_images[meraki]=0
rename_images[toonily]=0
# TODO save related metadata.
rename_images[imgur]=2
rename_images[imgursolo]=0

# $1: printf style format string to extract.
# $2: string to extract from
# %s => any string
# %d => any number
# %c => any character
# %^ => any string not including the next character
substring_format() {
	s="${1//\\/\\\\}" # / => \\
	s="${s//\//\\/}" # / => \/
	s="${s//./\\.}" # . => \.
	s="${s//\[/\\[}" # [ => \[
	s="$(sed 's/%^\(.\)/[^\1]\\+\1/g' <<< "$s")"
	s="${s//%s/.\\+}"
	s="${s//%d/[0-9]\\+}"
	s="${s//%c/.}"
	sed -n "s/.*\\($s\\).*/\\1/;T;p;q" <<< "$2"
}

# Site processing functions
# Read in initial webpage, and write out urls
extract_title() {
	title=$(sed -nr -e "/<title>/{s/.*<title>${name_regex[$site]}.*/\\1/;T;p;q}" <&10)
	[ "$title" ] || { err "unable to get name from url\\n"; return 1; }
}
extract_title2() {
	title=$(sed -nr -e "${name_regex[$site]}" <&10)
	[ "$title" ] || { err "unable to get name from url\\n"; return 1; }
}
extract_page_count() {
	pages=$(sed -nr "$1" <&10)
	[ "$pages" ] || { err "unable to get page count from url\\n"; return 1; }
}
output_urls_from_args() {
	local urls=("$@")
	pages=${#urls[@]}
	if [ "$pages" -eq 0 ]
	then err "Unable to parse links from $site\\n"; return 1
	else exec 11< <(printf '%s\n' "${urls[@]}")
	fi
}
output_urls_from_pages() {
	exec 11< <(
		for (( i=1; i <= "$pages"; ++i ))
		do curl -s "$1/$i" | sed -nr "$2"
		done
	)
}
output_urls_from_page() {
	local args
	mapfile -t args < <(sed -nr "$1" <&10)
	output_urls_from_args "${args[@]}"
}

# Site Processing function
process_gos_moe() {
	extract_title &&
	output_urls_from_page "/alt=\"$title/{s/.*src=\"([^\"]*)\".*/\\1/;p}"
}
process_webtoons() {
	extract_title &&
	output_urls_from_page '/class="_images"/{s/.*data-url="([^"]*)".*/\1/;p}'
}
process_mangacow() {
	extract_title &&
	exec 10< <(tr ';' '\n' <&10) &&
	output_urls_from_page '/arr_img.push/{s/.*"([^"]*)".*/\1/;p}'
}
process_mangastream() {
	extract_title &&
	extract_page_count '/a href.*Last Page/{s|.*/([0-9]*)">Last Page.*|\1|;p;q}' &&
	output_urls_from_pages "$(dirname "$link")" '/id="manga-page"/{s/.*src="([^"]*)".*/http:\1/;p}'
}
process_foolslide() {
	link="$(sed -r 's,/(|page/.*)$,/page/1,' <<< "$link")"
	extract_title &&
	extract_page_count '/tbtitle dropdown_parent dropdown_right/{s/.*>([0-9]*) .*/\1/;p;q}' &&
	output_urls_from_pages "$(dirname "$link")" '\,content/comics/,{s/.*src="([^"]*)".*/\1/;p}'
}
process_egscans() {
	# make use of webtoon javascript for images
	extract_title &&
	output_urls_from_page '/img_html.push.*picture/{s/.* src="([^"]*)".*/read.egscans.com\/\1/;p}'
}
process_fascans() {
	# make use of webtoon javascript for images
	extract_title2 &&
	output_urls_from_page "/data-src/{s/.*data-src=' ([^']*) '.*/\1/;/Credit/b;p}"
}
process_imgur() {
	title=$(basename "$link")
	local args
	mapfile -t args < <(perl -ne 'if (/item:/) { s/^.*"images"://; print "https://i.imgur.com/$1$2\n" while s|"hash":"([^"]*)".*?"ext":"([^"]*)"||; last }' <&10)
	output_urls_from_args "${args[@]}"
}
process_imgursolo() {
	title=$(basename "$link")
	local args
	mapfile -t args < <(perl -ne 'if (/item:/) { print "https://i.imgur.com/$1$2\n" if s|"hash":"([^"]*)".*?"ext":"([^"]*)"||; last }' <&10)
	output_urls_from_args "${args[@]}"
}
process_meraki() {
	local extract_pages='/var images/{s/.*(\[.*\]).*/\1/;p;q}'
	local base_url="$(sed 's,.com,.com/manga,' <<< $link)"
	# remove the first page, not part of manga
	local gen_url="del(.[0])[]|\"$base_url/\" + ."
	extract_title &&
	output_urls_from_args $(sed -nr "$extract_pages" <&10 | jq -r "$gen_url")
}
process_toonily() {
	extract_title && title+=" $(basename "$link")" &&
	output_urls_from_page '/WP-manga/{s/\s*([^"]*).*/\1/;p}'
}
#shellcheck disable=SC2155
#shellcheck disable=SC2046
process_mangadex() {
	extract_title || return 1
	local chapter="$(sed -r 's/https:..mangadex.org.chapter.([0-9]*).*/\1/' <<< "$link")"
	local query='
		if .server | contains("mangadex") or contains("cdndex")
		then .server + .hash + "/" + .page_array[]
		else "https://mangadex.org" + .server + .hash + "/" + .page_array[]
		end'
	local json="$(curl -s "https://mangadex.org/api/chapter/$chapter")"
	output_urls_from_args $(jq -r "$query" <<< "$json")
}


# If no argument read urls from stdin line by line
if [ "$#" -eq 0 ]
then
	# Queue of urls to download
	urls=()
	# continue if there are urls in queue, or if queue is empty
	# until we read the next url
	while [ "${#urls[@]}" -gt 0 ] || read urls[0]
	do

		# read from input and add jobs to queue
		while read -t 1 url
		do urls+=("$url");
		done
		if [ "${#urls[@]}" -gt 0 ]; then
			url="${urls[0]}"      # get first url
			unset urls[0]         # pop first url
			urls=("${urls[@]}")   # reorder urls (1,2,3 => 0,1,2)
			process_url "$url"
		fi
	done
else
	for url
	do process_url "$url"
	done
fi
exit $res
